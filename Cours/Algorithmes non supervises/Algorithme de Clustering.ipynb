{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1098725-96a0-4de9-b837-cc0efadfb2dd",
   "metadata": {},
   "source": [
    "## Evaluation de nos Modeles ##\n",
    "\n",
    "\n",
    "### Davies-Bouldin ###\n",
    "\n",
    "On peut utiliser le coefficient de Davies-Bouldin qui cherche a avoir des clusters homogenes (serres) et separes (distants). Il est donne par la relation *D = max (T1+Tk)/(Sk,l)* avec T pour l'heterogeneite et S pour la separation. On veut donc T petit (numerateur) et S grand (denominateur). Donc plus le coefficient est **petit, mieux c'est**.\n",
    "\n",
    "### Silouhette ###\n",
    "\n",
    "On peut aussi utiliser le coefficient de silouhette qui etablit la probabilite qu'un point x soit bien classé. Est-il proche de son cluster (distance moyenne a tous les autres points du cluster a(x)) et quelle est la plus petite valeur de a(x) si il etait assigne a un autre cluster (b(x)). On obtient *s(x) = (b(x)-a(x)) / max(a,b)*. Il est compris entre -1 et 1 et on le veut **proche de 1**. Il utilisable avec \"sklearn.metrics.silouhette_score\".\n",
    "\n",
    "### Stabilité ###\n",
    "\n",
    "En lancant l'algorithme plusieurs fois sur les memes donnees mais avec une initialisation differente, un sous ensemble ou des donnees bruitees, a-t-on les memes resultats? Tres important pour choisir le **nombre de cluster**. \n",
    "\n",
    "### Connaissances specifiques du domaine ###\n",
    "\n",
    "Par exemple si on classe des images pouvoir dire si elles sont classees selon le bon critere. Ca peut se voir a l'oeil facilement.\n",
    "\n",
    "On peut aussi utiliser la comparaison de la *concordnace de deux partitions* du jeux de donnes (cf sklearn.metrics). Par exemple, on peut utiliser l'indice de Rand (plutot le ARI, **Adjusted Rand Index**) qui regarde si un point va etre classe de la meme maniere dans des partitions aleatoires des donees. Il est egale a 0 pour de l'aleatoire complet et a **1 quand le clustering correspond a la partition initiale**. On peut l'avoir par \"sklearn.metrics.adjusted_rand_score\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9501a-eb7e-4dc4-b174-caf4e2f76ca0",
   "metadata": {},
   "source": [
    "## Optimisation des clusters ##\n",
    "\n",
    "### Via le clustering Hierarchique ###\n",
    "\n",
    "Pour bien separer les clusters sans prendre trop de temps de calcul, on peut faire du clustering hierarchique. On part d'un extreme (chaque point est un cluster ou tous les points sont dans un cluster) et on va graduellement vers l'autre extreme, la solution se trouvant forcement sur le chemin.\n",
    "\n",
    "Le *clustering de Ward* minimise l'augmentation de la variance (ditance au carre) inter cluster. Avec d'autres methodes de clustering on peut aussi chercher a maximiser la distance entre les clusters. (lien simple, complet, centroidal, moyen)\n",
    "\n",
    "Le clustering hierarchique se trouve dans le **module cluster de scikit-learn**.\n",
    "- Avantages : nombre de clusters non definis a l'avance.\n",
    "- Inconvenients : complexite algorithmique lourde.\n",
    "Le clustering hierarchique est donc plus adapte pour les echantillons avec un faible nombres d'individus.\n",
    "\n",
    "### Algorithme du k-means ###\n",
    "\n",
    "On cherche ici a minimiser la variance intra cluster.\n",
    "\n",
    "On utilise l'algorithme de Lloyd. Il converge rapidement mais peut etre dans un minimum local **donc** relancer plusieurs fois et evaluer la variance intra cluster a chaque fois pour comparer. Utilisable daans **sklearn.cluster.KMeans**. Par defaut l'algorithme est repete dix fois (n_init=10).\n",
    "\n",
    "- Un probleme va etre que chaque point ayant le centroide le plus proche, on a que des clusters convexes (pas de croissant ou d'anneau) selon une tessellation de Voronoi. On peut palier grace a un kernel k-means utilisable via **sklearn.cluster.KernelMeans**.\n",
    "\n",
    "- Un autre probleme va etre que l'initialisation se fait au hasard et on peut avoir des resultats tres differents et parfois tres mauvais. On va y pallier avec le k-means++ qui eparpille le plus possible les donnees avec les centroides initiaux. **sklearn.cluster.KMeans** se fait par defaut avec kmeans++ (parametre 'init').\n",
    "\n",
    "### Partition avec DBSCAN (Density-Based Spatial Clustering of Applications with Noise) ###\n",
    "\n",
    "On va utiliser le DBSCAN pour former des clusters non convexe (demi-lunes, cercles imbriques, etc). C'est un clustering par densite. On peut relier les points de proches en proches en restant dans le meme cluster.\n",
    "\n",
    "- Avantages : Temps de calcul court sans predefinir le nombre de cluster, cluster de forme arbitraire possible.\n",
    "- Inconvenients : difficile a utiliser en tres grande dimension (fleau de la dimensionnalite), choix des parametres epsilon et nmin delicat. (avoir assez de points interieurs) donc les clusters ont tous la meme densite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e0a29-a281-449b-8ade-0aa3c933f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithme du DBSCAN :\n",
    "\n",
    "1. Prendre un point x qui n’a pas été visité\n",
    "\n",
    "2. Construire N = voisinage(eps, x)\n",
    "\n",
    "3.  If |N| < n_min:\n",
    "\n",
    "        Marquer x comme bruit\n",
    "\n",
    "    Else:\n",
    "\n",
    "        Initaliser C = {x}\n",
    "        agrandir_cluster(C, N, eps, n_min)\n",
    "\n",
    "    Ajouter C à la liste des clusters\n",
    "    Marquer tous les points de C comme visités\n",
    "\n",
    "4. Repeat 1-3 until tous les points ont été visités\n",
    "\n",
    "# Puis pour agrandir_cluster :\n",
    "\n",
    "agrandir_cluster(C, N, eps, n_min):\n",
    "    For u in N:\n",
    "        If u n’a pas été visité:\n",
    "            N’ = N(eps, u)\n",
    "        If |N’| >= n_min:\n",
    "            N = union(N, N’)\n",
    "        If u n’appartient à aucun autre cluster:\n",
    "            Ajouter u à C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
